{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c55ee8-c18f-4fbe-92f9-eab02bb58e57",
   "metadata": {},
   "source": [
    "# Get Started with Generative AI\n",
    "\n",
    "Generative AI is a type of artificial intelligence that can create new content and ideas, including conversations, stories, images, videos, and music. Like all artificial intelligence, generative AI is powered by machine learning modelsâ€”very large models that are pre-trained on vast amounts of data and commonly referred to as foundation models (FMs). Apart from content creation, generative AI is also used to improve the quality of digital images, edit video, build prototypes quickly for manufacturing, augment data with synthetic datasets, and more.\n",
    "\n",
    "## Using DeepSeek models \n",
    "\n",
    "---\n",
    "This demo notebook shows how to interact with a deployed DeepSeek model endpoint on Amazon SageMaker AI by using the SageMaker Python SDK for text generation. DeepSeek models are known for their strong performance, particularly in coding and reasoning tasks. We show several example prompt engineering use cases, including code generation, question answering, and controlled model output.\n",
    "\n",
    "Note: This notebook assumes you've already deployed a DeepSeek model to a SageMaker endpoint. You connect to this existing endpoint.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf04a9b-2da8-45ae-9346-4697937318a7",
   "metadata": {},
   "source": [
    "### Model details\n",
    "\n",
    "---\n",
    "DeepSeek LLM is a family of models developed by DeepSeek AI. The models come in various sizes and are trained on large datasets, with a significant portion dedicated to code, making them adept at programming-related tasks. \n",
    "\n",
    "This notebook focuses on interacting with a predeployed endpoint. For details on the specific DeepSeek model version, training data, and potential limitations (such as language support or inherent biases), refer to the model's documentation or the SageMaker JumpStart page used for its deployment.\n",
    "\n",
    "DeepSeek models often include the following characteristics:\n",
    "- Provide strong coding and mathematical reasoning capabilities\n",
    "- Available in base and instruction-tuned/chat variants\n",
    "- Trained on a diverse datasets, including web text and code\n",
    "\n",
    "DeepSeek models include the following limitations:\n",
    "- Like most large language models (LLMs), DeepSeek models can inherit biases from their training data. Use guardrails and appropriate precautions for production use.\n",
    "- Performance can vary across different languages or highly specialized domains not well-represented in the training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be338dbb-57e7-434e-85d3-1427f5975f96",
   "metadata": {},
   "source": [
    "## Connect to the deployed DeepSeek endpoint\n",
    "Instead of deploying a model in this practice lab, you connect to an existing SageMaker endpoint that hosts the DeepSeek model. You need to provide the name of your specific endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3763cc4c-modified",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:13:55.633905Z",
     "iopub.status.busy": "2025-06-17T13:13:55.633494Z",
     "iopub.status.idle": "2025-06-17T13:13:58.895909Z",
     "shell.execute_reply": "2025-06-17T13:13:58.895104Z",
     "shell.execute_reply.started": "2025-06-17T13:13:55.633879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Connecting to endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247...\n",
      "Successfully connected to endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import retrieve_default\n",
    "import json \n",
    "import boto3\n",
    "\n",
    "# --- IMPORTANT --- \n",
    "# Replace this with the actual name of the deployed DeepSeek endpoint\n",
    "endpoint_name = \"jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\" \n",
    "# ----------------- \n",
    "\n",
    "predictor = None # Initialize predictor\n",
    "try:\n",
    "    print(f\"Connecting to endpoint: {endpoint_name}...\")\n",
    "    # Use retrieve_default which automatically handles serializers/deserializers for known JumpStart containers\n",
    "    predictor = retrieve_default(endpoint_name)\n",
    "    print(f\"Successfully connected to endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Error] connecting to endpoint {endpoint_name}: {e}\")\n",
    "    print(\"Please ensure the endpoint name is correct and the endpoint is in 'InService' status.\")\n",
    "    # Optionally raise the error or handle it as needed\n",
    "    # raise e \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0286aa2-9c12-44a2-941f-1f1d808890c4",
   "metadata": {},
   "source": [
    "### Supported parameters (example for DeepSeek chat)\n",
    "\n",
    "DeepSeek models deployed through SageMaker AI often accept parameters within a JSON payload. The exact parameters depend on the specific deployment container (for example, TGI, vLLM, and DJL). Based on the reference code, a common structure for chat/instruct models involves a `messages` list and other generation controls. Common parameters include:\n",
    "\n",
    "* **messages:** A list of message objects, each with `role` ('user', 'assistant', or sometimes 'system') and `content` (the text of the message). This allows for conversational context.\n",
    "* **max_tokens** (or `max_new_tokens`): Maximum number of tokens to generate in the response.\n",
    "* **temperature:** Controls randomness. Lower values (0.1-0.3) make the output more focused and deterministic; higher values (0.7-1.0) make it more creative and diverse.\n",
    "* **top_p:** Nucleus sampling parameter. Considers only the most probable tokens whose cumulative probability exceeds `top_p`.\n",
    "* **stop:** A list of strings. Generation stops if the model produces any of these strings.\n",
    "\n",
    "Refer to the documentation of your specific SageMaker AI deployment (for example, TGI container images or the SageMaker JumpStart model card) for the definitive list of supported parameters and payload structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bd778-5c62-4757-80ce-38c29275fa2a",
   "metadata": {},
   "source": [
    "## Create a query endpoint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8eb4f7-65b8-428e-8d4f-ca8849228d56-modified",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-17T13:18:15.859761Z",
     "iopub.status.busy": "2025-06-17T13:18:15.859309Z",
     "iopub.status.idle": "2025-06-17T13:18:15.873218Z",
     "shell.execute_reply": "2025-06-17T13:18:15.872173Z",
     "shell.execute_reply.started": "2025-06-17T13:18:15.859731Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Assume 'predictor' object and 'endpoint_name' variable exist from previous cells\n",
    "\n",
    "def query_endpoint(prompt, temperature=0.7, max_tokens=10240):\n",
    "    \"\"\"\n",
    "    This function handles sending the request, parsing the response to find \n",
    "    both the model's reasoning (thought process) and the final answer, \n",
    "    and prints them separately for clarity.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to send to the model.\n",
    "        temperature (float): Controls the randomness of the output (0.0 to ~1.0).\n",
    "                             Lower values are more focused, higher are more creative.\n",
    "        max_tokens (int): The maximum number of tokens (words/subwords) the model\n",
    "                          should generate in its response.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The full response dictionary from the endpoint, or None if an error occurred.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n--> Sending prompt to endpoint: {endpoint_name}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    # Show the input prompt clearly\n",
    "    print(f\"[Input Prompt]:\\n{prompt}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    # 1. Prepare the data payload (the 'package' you send to the model)\n",
    "    #    DeepSeek chat models expect data in a specific format:\n",
    "    #    - A 'messages' list containing one or more message dictionaries.\n",
    "    #    - Each message dictionary has a 'role' ('user' for us, 'assistant' for the model)\n",
    "    #      and 'content' (the actual text of the message).\n",
    "    #    - You also include parameters such as 'max_tokens' and 'temperature'.\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt} # Our prompt goes here\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,         # Max length of the model's reply\n",
    "        \"temperature\": temperature      # Controls creativity compared to focus\n",
    "        # Other parameters, such as 'top_p', could be added here if needed\n",
    "    }\n",
    "\n",
    "    # Initialize variables to store parts of the response later\n",
    "    raw_response = None     # To store the entire response dictionary\n",
    "    reasoning = None        # To store the model's thought process\n",
    "    final_answer = None     # To store the model's final answer\n",
    "    warning_message = None  # To store any warnings (like truncation)\n",
    "\n",
    "    try:\n",
    "        # 2. Send the payload to the SageMaker endpoint by using the 'predictor'\n",
    "        #    The 'predictor.predict()' function handles the communication.\n",
    "        #    We need to make sure the 'predictor' object was successfully created earlier.\n",
    "        if 'predictor' not in globals() or predictor is None:\n",
    "             print(\"[Error]: 'predictor' object not found. \\nDid the connection to the endpoint in the previous cell succeed?\")\n",
    "             return None # Stop the function here if predictor doesn't exist\n",
    "             \n",
    "        print(\"Waiting for response from the model...\")\n",
    "        raw_response = predictor.predict(payload)\n",
    "\n",
    "        # 3. Process the response received from the endpoint\n",
    "        #    The response is usually a dictionary that contains details about the generation.\n",
    "        #    You need to carefully look inside it to find the generated text.\n",
    "\n",
    "        # Use '.get(key)' which is safer than '[key]' as it returns None if the key doesn't exist,\n",
    "        # preventing errors if the response structure is slightly different.\n",
    "        if isinstance(raw_response, dict):\n",
    "            choices = raw_response.get('choices')\n",
    "            # Check if 'choices' exists and is a list with at least one item\n",
    "            if isinstance(choices, list) and len(choices) > 0:\n",
    "                first_choice = choices[0] # Get the first (usually only) choice\n",
    "                if isinstance(first_choice, dict):\n",
    "                    message = first_choice.get('message') # Get the 'message' dictionary\n",
    "                    if isinstance(message, dict):\n",
    "                        # Try to get the model's reasoning (thought process)\n",
    "                        reasoning = message.get('reasoning_content')\n",
    "                        # Try to get the final answer/content\n",
    "                        final_answer = message.get('content')\n",
    "\n",
    "                    # Check if the response was cut short (truncated)\n",
    "                    finish_reason = first_choice.get('finish_reason')\n",
    "                    if finish_reason == 'length':\n",
    "                        warning_message = f\"[Warning]: The model's output might have been cut short because it reached the maximum token limit ({max_tokens}). You might need to increase 'max_tokens' for a longer response.\"\n",
    "        \n",
    "        # --- If you couldn't find the expected text parts --- \n",
    "        if final_answer is None and reasoning is None:\n",
    "             # Append to existing warning or create new one\n",
    "             if warning_message:\n",
    "                 warning_message += \"\\n[Warning]: Could not extract text using keys 'reasoning_content' or 'content'.\"\n",
    "             else:\n",
    "                 warning_message = \"[Warning]: Could not extract text using keys 'reasoning_content' or 'content'.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors that might happen during the prediction \n",
    "        # (for example, network problems, errors from the endpoint itself)\n",
    "        print(f\"\\n[Error] Occurred while querying the endpoint: {e}\")\n",
    "        try:\n",
    "             # Show the data you tried to send (helps find problems)\n",
    "             print(f\"   Payload attempted: {json.dumps(payload)}\")\n",
    "        except TypeError:\n",
    "             print(\"   Payload attempted: (Contains non-serializable types)\")\n",
    "        if raw_response is not None:\n",
    "             # Show the raw response if you received anything before the error\n",
    "             print(f\"   Raw response received before error: {raw_response}\")\n",
    "        return None # Indicate function failed by returning None\n",
    "\n",
    "    # 4. Print the results in a structured and easy-to-read way\n",
    "\n",
    "    print(\"\\n<-- Received response:\")\n",
    "    print(\"====================================================\")\n",
    "\n",
    "    # Print any warnings first\n",
    "    if warning_message:\n",
    "        print(f\"{warning_message}\\n\")\n",
    "\n",
    "    # Print the reasoning/thought process if it was found\n",
    "    # Check if reasoning is not None and is not empty string\n",
    "    if reasoning and reasoning.strip():\n",
    "        print(\"[Model's Reasoning (Thought Process)]:\")\n",
    "        print(reasoning.strip()) # .strip() removes leading/trailing whitespace\n",
    "        print(\"----------------------------------------------------\")\n",
    "    else:\n",
    "        # Inform the user if reasoning wasn't found or was empty\n",
    "        print(\"(No 'reasoning_content' found or it was empty in the response)\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "\n",
    "    # Print the final answer if it was found\n",
    "    # Check if final_answer is not None and is not empty string\n",
    "    if final_answer and final_answer.strip():\n",
    "        print(\"[Final Answer]:\")\n",
    "        print(final_answer.strip())\n",
    "        print(\"====================================================\")\n",
    "    else:\n",
    "        # Inform the user if the final answer wasn't found or was empty\n",
    "        print(\"[Error] (No final 'content' found or it was empty in the response)\")\n",
    "        print(\"====================================================\")\n",
    "        # If *both* parts were effectively missing, show the raw response to help debug\n",
    "        if not (reasoning and reasoning.strip()):\n",
    "             print(\"\\nRaw Response Received (for debugging, as key content seems missing):\")\n",
    "             # Pretty print the JSON for better readability\n",
    "             try:\n",
    "                 print(json.dumps(raw_response, indent=2))\n",
    "             except Exception:\n",
    "                 print(raw_response) # Fallback if JSON formatting fails\n",
    "\n",
    "    # 5. Return the complete raw response dictionary\n",
    "    #    This allows the code calling this function to inspect other details\n",
    "    #    such as token usage ('usage' dictionary) if needed.\n",
    "    return raw_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a9caa-5dfd-4a34-a56f-469468d0ba44",
   "metadata": {},
   "source": [
    "# Prompt engineering techniques\n",
    "\n",
    "Now, it's time to test the same prompt engineering techniques by using the connected DeepSeek model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4585b9f-eb15-4f8c-bffe-d1807f0f66b3",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "In zero-shot prompting, you ask the model to perform a task without any examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e3d263-f600-4354-bb04-f117b0e55440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:19:06.655613Z",
     "iopub.status.busy": "2025-06-17T13:19:06.654237Z",
     "iopub.status.idle": "2025-06-17T13:19:12.253044Z",
     "shell.execute_reply": "2025-06-17T13:19:12.252258Z",
     "shell.execute_reply.started": "2025-06-17T13:19:06.655579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Sending prompt to endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\n",
      "----------------------------------------------------\n",
      "[Input Prompt]:\n",
      "\n",
      "Write a program to compute factorial in Python.\n",
      "\n",
      "----------------------------------------------------\n",
      "Waiting for response from the model...\n",
      "\n",
      "<-- Received response:\n",
      "====================================================\n",
      "[Model's Reasoning (Thought Process)]:\n",
      "To compute the factorial of a number in Python, I can create a function that takes an integer as input and returns the product of all integers from 1 up to that number. \n",
      "\n",
      "I should handle the case where the input is 0, as 0! is defined to be 1. \n",
      "\n",
      "For positive integers, I'll loop from 1 to the input number, multiplying each value to accumulate the result. \n",
      "\n",
      "For negative integers, since factorials are only defined for non-negative integers, I'll add a check to return 0 if the input is negative.\n",
      "\n",
      "I'll test the function with a few examples to ensure it works correctly and efficiently.\n",
      "----------------------------------------------------\n",
      "[Final Answer]:\n",
      "To compute the factorial of a number in Python, you can write a function that takes an integer as input and returns the product of all positive integers up to that number. Here's a step-by-step guide:\n",
      "\n",
      "### Step 1: Define the Function\n",
      "Create a function named `factorial` that accepts an integer `n` as its parameter.\n",
      "\n",
      "### Step 2: Handle Base Cases\n",
      "- If `n` is 0 or 1, return 1 because 0! = 1 and 1! = 1.\n",
      "- If `n` is negative, return 0 because factorial is not defined for negative numbers.\n",
      "\n",
      "### Step 3: Compute the Factorial for Positive Integers\n",
      "For positive integers greater than 1, initialize a variable `result` to 1. Then loop from 1 to `n` (inclusive), multiplying each value to `result`.\n",
      "\n",
      "### Step 4: Return the Result\n",
      "Return the computed `result`.\n",
      "\n",
      "### Example Code\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        return 0\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "# Example usage\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(0))  # Output: 1\n",
      "print(factorial(-3)) # Output: 0\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "1. **Base Cases:** The function immediately returns 1 if `n` is 0 or 1.\n",
      "2. **Negative Input:** If `n` is negative, the function returns 0.\n",
      "3. **Loop Calculation:** For non-negative integers greater than 1, the function initializes `result` to 1 and iterates from 2 to `n`, multiplying each value to `result`.\n",
      "4. **Return Value:** Finally, the accumulated product is returned.\n",
      "\n",
      "This approach efficiently computes the factorial using a simple loop, ensuring correctness for all non-negative integers and handling edge cases appropriately.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Write a program to compute factorial in Python.\n",
    "\"\"\"\n",
    "# Use the updated query function\n",
    "raw_response = query_endpoint(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b17a2-aeee-45af-8c47-a3896a0e67b6",
   "metadata": {},
   "source": [
    "## One-shot prompting\n",
    "In one-shot prompting, you provide one example to guide the model. \n",
    "\n",
    "Note: For chat models such as DeepSeek, structuring this example within the `messages` list as a user/assistant pair might be more effective than including it directly in the user prompt, but the original structure is kept in this practice lab for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed72a938-3704-4bc4-98fa-3cf87d7a8862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T14:29:18.212754Z",
     "iopub.status.busy": "2025-06-17T14:29:18.212444Z",
     "iopub.status.idle": "2025-06-17T14:29:20.657868Z",
     "shell.execute_reply": "2025-06-17T14:29:20.655778Z",
     "shell.execute_reply.started": "2025-06-17T14:29:18.212730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Sending prompt to endpoint: jumpstart-dft-llama-3-2-1b-instruct-20250617-140458\n",
      "----------------------------------------------------\n",
      "[Input Prompt]:\n",
      "\n",
      "Here's an example of an AWS Lambda function that generates weather forecasts:\n",
      "\n",
      "```python\n",
      "import json\n",
      "import random\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    try:\n",
      "        weather_types = [\"sunny\", \"rainy\", \"cloudy\"]\n",
      "        forecast = random.choice(weather_types)\n",
      "        return {\n",
      "            'statusCode': 200,\n",
      "            'body': json.dumps({'forecast': forecast})\n",
      "        }\n",
      "    except Exception as e:\n",
      "        return {\n",
      "            'statusCode': 500,\n",
      "            'body': json.dumps({'error': str(e)})\n",
      "        }\n",
      "```\n",
      "\n",
      "Now, write a similar Lambda function that generates random colors (e.g., red, green, blue).\n",
      "\n",
      "----------------------------------------------------\n",
      "Waiting for response from the model...\n",
      "\n",
      "<-- Received response:\n",
      "====================================================\n",
      "(No 'reasoning_content' found or it was empty in the response)\n",
      "----------------------------------------------------\n",
      "[Final Answer]:\n",
      "Here's an example of an AWS Lambda function that generates random colors:\n",
      "\n",
      "```python\n",
      "import json\n",
      "import random\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    \"\"\"\n",
      "    Generates a random color and returns it as a JSON object.\n",
      "\n",
      "    Args:\n",
      "        event (dict): Not used in this example.\n",
      "        context (dict): Not used in this example.\n",
      "\n",
      "    Returns:\n",
      "        dict: A JSON object containing a random color.\n",
      "    \"\"\"\n",
      "    # Define a list of possible colors\n",
      "    colors = [\"red\", \"green\", \"blue\", \"black\", \"white\"]\n",
      "\n",
      "    # Choose a random color from the list\n",
      "    color = random.choice(colors)\n",
      "\n",
      "    # Convert the color to a hex code\n",
      "    hex_code = \"#{:02X}{:02X}{:02X}\".format(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
      "\n",
      "    # Return the random color as a JSON object\n",
      "    return {\n",
      "        'statusCode': 200,\n",
      "        'body': json.dumps({'color': color, 'hex_code': hex_code})\n",
      "    }\n",
      "```\n",
      "\n",
      "This Lambda function uses the same basic approach as the previous one, but instead of returning a random forecast, it generates a random color and returns it as a JSON object. The color is represented as a hexadecimal code (e.g., `#FF0000` for red). The function then returns a JSON object containing the color and its hexadecimal code.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = \"\"\"\n",
    "Here's an example of an AWS Lambda function that generates weather forecasts:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import random\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        weather_types = [\"sunny\", \"rainy\", \"cloudy\"]\n",
    "        forecast = random.choice(weather_types)\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({'forecast': forecast})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "```\n",
    "\n",
    "Now, write a similar Lambda function that generates random colors (e.g., red, green, blue).\n",
    "\"\"\"\n",
    "# Use the updated query function\n",
    "raw_response =  query_endpoint(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902acc-ca1e-4af1-9e94-f63be5bd00cc",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "In few-shot prompting, you provide multiple examples to establish a pattern.\n",
    "\n",
    "Note: Similar to one-shot prompting, providing these examples as distinct user/assistant turns in the `messages` list is often better for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2900b065-aa54-414a-9ba1-6dc5ff47d71b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T14:30:27.552960Z",
     "iopub.status.busy": "2025-06-17T14:30:27.552086Z",
     "iopub.status.idle": "2025-06-17T14:30:32.177565Z",
     "shell.execute_reply": "2025-06-17T14:30:32.176613Z",
     "shell.execute_reply.started": "2025-06-17T14:30:27.552772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Sending prompt to endpoint: jumpstart-dft-llama-3-2-1b-instruct-20250617-140458\n",
      "----------------------------------------------------\n",
      "[Input Prompt]:\n",
      "\n",
      "Example 1 - Simple Lambda:\n",
      "```python\n",
      "def lambda_handler(event, context):\n",
      "    return {'message': 'Hello World'}\n",
      "```\n",
      "\n",
      "Example 2 - Lambda with error handling:\n",
      "```python\n",
      "import json\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    try:\n",
      "        # Your logic here\n",
      "        result = 'Success'\n",
      "        return {'statusCode': 200, 'body': json.dumps({'message': result})}\n",
      "    except Exception as e:\n",
      "        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}\n",
      "```\n",
      "\n",
      "Example 3 - Lambda with logging:\n",
      "```python\n",
      "import logging\n",
      "import json\n",
      "\n",
      "logger = logging.getLogger()\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    logger.info(f'Processing event: {event}')\n",
      "    try:\n",
      "        # Your logic here\n",
      "        result = 'Logged and Processed'\n",
      "        return {'statusCode': 200, 'body': json.dumps({'message': result})}\n",
      "    except Exception as e:\n",
      "        logger.error(f'Error processing event: {e}')\n",
      "        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}\n",
      "```\n",
      "\n",
      "Now, create a Lambda function that:\n",
      "1. Imports necessary libraries (random, logging, json)\n",
      "2. Includes proper error handling (try/except block)\n",
      "3. Implements logging for requests and errors\n",
      "4. Defines a list of quotes from Greek philosophers (e.g., Socrates, Plato, Aristotle)\n",
      "5. Randomly selects and returns one quote in the JSON body\n",
      "6. Includes basic docstrings and comments\n",
      "7. Returns a 200 status code on success and 500 on error.\n",
      "\n",
      "----------------------------------------------------\n",
      "Waiting for response from the model...\n",
      "\n",
      "<-- Received response:\n",
      "====================================================\n",
      "(No 'reasoning_content' found or it was empty in the response)\n",
      "----------------------------------------------------\n",
      "[Final Answer]:\n",
      "Here's the code based on the provided specifications:\n",
      "\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import json\n",
      "import logging\n",
      "import random\n",
      "\n",
      "# Set up logging\n",
      "logger = logging.getLogger()\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "# Define a list of quotes from Greek philosophers\n",
      "greek_quotes = [\n",
      "    'The unexamined life is not worth living.',\n",
      "    'All knowledge is recollection.',\n",
      "    'Wealth consists not in having great riches, but in having great goods, and in being able to get them with indifference.',\n",
      "    'Not everything that can be counted counts, and not everything that counts can be counted.',\n",
      "    'The whole is more than the sum of its parts.',\n",
      "    'To love oneself is the beginning of a lifelong romance.',\n",
      "    'The unexamined teacher is not worth listening to.',\n",
      "    'The universe is not only stranger than we think, it is stranger than we can think.',\n",
      "    'The unexamined life is not worth living.',\n",
      "    'The greatest glory in living lies not in never falling, but in rising every time we fall.',\n",
      "    'The test of our character is not how well we treat others, but how well we treat ourselves.',\n",
      "    'Learning is not the filling of a pail, but the lighting of a fire.',\n",
      "    'I can resist everything except temptation.',\n",
      "    'You can't go back and change the beginning, but you can start where you are and change the ending.',\n",
      "    'The only way to do great work is to love what you do.',\n",
      "    'Success is not final, failure is not fatal: It is the courage to continue that counts.',\n",
      "]\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    \"\"\"\n",
      "    The main function that handles the Lambda event.\n",
      "\n",
      "    Args:\n",
      "        event (dict): The event that triggered this Lambda function.\n",
      "        context (dict): The execution context for the Lambda function.\n",
      "\n",
      "    Returns:\n",
      "        dict: The response to the Lambda function, including a 200 status code and a JSON body.\n",
      "    \"\"\"\n",
      "\n",
      "    # Log the request\n",
      "    logger.info(f'Processing event: {event}')\n",
      "\n",
      "    try:\n",
      "        # Select a random quote from the list\n",
      "        quote = random.choice(greek_quotes)\n",
      "\n",
      "        # Return the response with the random quote\n",
      "        return {\n",
      "            'statusCode': 200,\n",
      "            'body': json.dumps({'quote': quote})\n",
      "        }\n",
      "    except Exception as e:\n",
      "        # Log the error and return a 500 status code\n",
      "        logger.error(f'Error processing event: {e}')\n",
      "        return {\n",
      "            'statusCode': 500,\n",
      "            'body': json.dumps({'error': str(e)})\n",
      "        }\n",
      "```\n",
      "\n",
      "Note that I've used `random.choice` to select a random quote from the list. You can replace this with your own logic to select a quote. Also, I've kept the comments and docstrings as per the specifications.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Example 1 - Simple Lambda:\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    return {'message': 'Hello World'}\n",
    "```\n",
    "\n",
    "Example 2 - Lambda with error handling:\n",
    "```python\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Your logic here\n",
    "        result = 'Success'\n",
    "        return {'statusCode': 200, 'body': json.dumps({'message': result})}\n",
    "    except Exception as e:\n",
    "        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}\n",
    "```\n",
    "\n",
    "Example 3 - Lambda with logging:\n",
    "```python\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    logger.info(f'Processing event: {event}')\n",
    "    try:\n",
    "        # Your logic here\n",
    "        result = 'Logged and Processed'\n",
    "        return {'statusCode': 200, 'body': json.dumps({'message': result})}\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error processing event: {e}')\n",
    "        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}\n",
    "```\n",
    "\n",
    "Now, create a Lambda function that:\n",
    "1. Imports necessary libraries (random, logging, json)\n",
    "2. Includes proper error handling (try/except block)\n",
    "3. Implements logging for requests and errors\n",
    "4. Defines a list of quotes from Greek philosophers (e.g., Socrates, Plato, Aristotle)\n",
    "5. Randomly selects and returns one quote in the JSON body\n",
    "6. Includes basic docstrings and comments\n",
    "7. Returns a 200 status code on success and 500 on error.\n",
    "\"\"\"\n",
    "\n",
    "# Use the updated query function, keeping max_tokens high for longer code\n",
    "# Increased max_tokens slightly more just in case\n",
    "raw_response = query_endpoint(few_shot_prompt, max_tokens=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85354676-4ae1-4063-b5e4-0eb245e95155",
   "metadata": {},
   "source": [
    "## Use clear and specific instructions\n",
    "\n",
    "Prompt engineering techniquesâ€”such as zero-shot, one-shot, and few-shot approachesâ€”offer different ways to guide AI models to produce desired outputs. Zero-shot prompting is the most basic approach, where you directly ask the model to perform a task without examples. Zero-shot prompting is useful for straightforward requests, but it might lack precision for complex tasks. One-shot prompting provides a single example along with the request, which helps the model understand the expected format and style of the response. Few-shot prompting takes this further by providing multiple examples, which is particularly effective for complex tasks that require pattern recognition or specific output structures.\n",
    "\n",
    "Clear instructions are crucial across all these techniques because they reduce ambiguity and help the model understand exactly what's expected. When instructions are specific, well-structured, and include details about the desired format, context, and constraints, the model is more likely to generate accurate and relevant responses. For example, instead of asking the model to \"generate some code,\" a clear instruction might specify that the model should \"write a Python function that handles errors, includes logging, and returns JSON responses with specific fields.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c694cb4f-2672-4af7-9613-dc671e71024e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:26:08.934858Z",
     "iopub.status.busy": "2025-06-17T13:26:08.934272Z",
     "iopub.status.idle": "2025-06-17T13:26:20.661461Z",
     "shell.execute_reply": "2025-06-17T13:26:20.660172Z",
     "shell.execute_reply.started": "2025-06-17T13:26:08.934828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Sending prompt to endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\n",
      "----------------------------------------------------\n",
      "[Input Prompt]:\n",
      "\n",
      "Create an AWS Lambda function in Python with the following specific requirements:\n",
      "1.  Function name should ideally be `greek_philosopher_quote_lambda` (inside the handler code).\n",
      "2.  It must include a list containing quotes from at least these three philosophers: Socrates, Plato, Aristotle.\n",
      "3.  Each quote in the list should ideally be stored as a dictionary or tuple, pairing the quote text with the philosopher's name (e.g., {'philosopher': 'Socrates', 'quote': 'An unexamined life is not worth living.'}).\n",
      "4.  The function should randomly select one quote object (philosopher and quote) from the list.\n",
      "5.  It must include error handling using a try/except block to catch potential issues during execution.\n",
      "6.  It must implement logging using the `logging` library to log the start of the function execution and any errors encountered.\n",
      "7.  The function must return a dictionary suitable for API Gateway proxy integration, specifically:\n",
      "    - On success: `{'statusCode': 200, 'body': json.dumps({'philosopher': selected_philosopher, 'quote': selected_quote})}`\n",
      "    - On error: `{'statusCode': 500, 'body': json.dumps({'error': 'An error occurred'})}` or a more specific error message.\n",
      "8. Include basic docstrings for the function and comments where necessary.\n",
      "\n",
      "----------------------------------------------------\n",
      "Waiting for response from the model...\n",
      "\n",
      "<-- Received response:\n",
      "====================================================\n",
      "[Model's Reasoning (Thought Process)]:\n",
      "Okay, so I have this task where I need to create an AWS Lambda function in Python based on the user's requirements. Let me go through each point step by step to make sure I understand what's needed.\n",
      "\n",
      "First, the function name is specified as `greek_philosopher_quote_lambda`, which is a bit of a name, but I guess it's just a placeholder. The main job of this function is to retrieve quotes from three famous Greek philosophers: Socrates, Plato, and Aristotle. Each quote should be stored as a dictionary with the philosopher's name and the quote text.\n",
      "\n",
      "I also need to import the necessary libraries. The user mentioned logging, so I'll need to import the logging module. Plus, using the requests library for HTTP requests and json for JSON operations. So I'll include those imports at the top.\n",
      "\n",
      "Next, the function should randomly select one quote object from the list. Each quote is a dictionary of the form {'philosopher': 'Socrates', 'quote': 'An unexamined life is not worth living.'}. So I'll need to store the list of these quote dictionaries somewhere, probably in a variable.\n",
      "\n",
      "Error handling is a must. I'll use try/except blocks to catch any exceptions that might occur during the function's execution, such as network errors or requests failing. This will log the error and return a 500 status code with an appropriate message.\n",
      "\n",
      "Logging is another key point. I'll import the logging library and set up the function to log the start of execution using logging.info. Also, during errors, logging.error will be useful.\n",
      "\n",
      "The success outcome should return a JSON-dumped dictionary with the selected philosopher and quote. For errors, it should log that an error occurred and return a 500 status with the error message.\n",
      "\n",
      "I should include comments where necessary to explain the code, but since the user asked for basic docstrings and comments, I'll write a docstring explaining the function's purpose, parameters, return values, and note about including the .env file and AWS setup in production.\n",
      "\n",
      "Now, thinking about the structure of the function. It will start by logging the function's start. Then, in a try block, it will generate a list of three random quote dictionaries. It'll use a for loop iterating three times, each time selecting a quote with random selection. After that, it will randomly select one quote from the list.\n",
      "\n",
      "If everything goes well, it logs the selected quotes and returns the JSON-dumped dictionary. If there's an error, it catches the exception, logs the error, and returns the 500 status with the message.\n",
      "\n",
      "I should also consider edge cases. For example, what if the API endpoints aren't correctly set up? The function should handle that gracefully but inform the user that it didn't work.\n",
      "\n",
      "Wait, the user mentioned the .env file and AWS setup, but since I'm writing this as a script, maybe I should note that in a comment. They also asked for basic docstrings and comments, which I've done.\n",
      "\n",
      "Let me structure the code. Start with the imports. Then, the function definition. Inside the function, log the start. Generate the quote list, select one, check for errors. Handle exceptions, log errors, and return the appropriate response.\n",
      "\n",
      "I think that covers all the points. Now, I'll write the code accordingly, making sure to handle all the requirements mentioned.\n",
      "----------------------------------------------------\n",
      "[Final Answer]:\n",
      "```python\n",
      "import logging\n",
      "import requests\n",
      "import json\n",
      "\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logging.getLogger(__name__)..setLevel(logging.DEBUG)\n",
      "\n",
      "def greek_philosopher_quote_lambda():\n",
      "    \"\"\"\n",
      "    Function to retrieve and random select Greek philosophical quotes.\n",
      "    \n",
      "    Returns:\n",
      "        {success: {status: 200, body: {philosopher: \"Socrates\", quote: ...}}}\n",
      "        {error: {status: 500, body: {error: ...}}}\n",
      "    \n",
      "    Raises:\n",
      "        Exception: If network errors occur or requests fail to authenticate.\n",
      "    \"\"\"\n",
      "    # Initialize logging\n",
      "    logging.info(\"Starting Greek philosophical quotes function\")\n",
      "    \n",
      "    # List of philosopher quote pairs\n",
      "    quote_list = [\n",
      "        {'philosopher': 'Socrates', 'quote': 'An unexamined life is not worth living.'},\n",
      "        {'philosopher': 'Plato', 'quote': 'Let no one who is without a vision of an ideal society enter my door.'},\n",
      "        {'philosopher': 'Aristotle', 'quote': 'The unexamined life is not worth considering: Only the examined life is\n",
      "worth living.'}\n",
      "    ]\n",
      "    \n",
      "    try:\n",
      "        # Randomly select one quote\n",
      "        selected_quote = random.choice(quote_list)\n",
      "        \n",
      "        # Generate three random quotes\n",
      "        random_quotes = [random.choice(quote_list) for _ in range(3)]\n",
      "        selected_quotes = random.sample(random_quotes, k=len(quote_list))[:3]\n",
      "        \n",
      "        # Select one quote randomly\n",
      "        selected_quote = random.choice(selected_quotes)\n",
      "        \n",
      "        logging.info(f\"Successfully retrieved and selected a quote: {selected_quote['philosopher']} - {selected_quote['quote']}\")\n",
      "        return json.dumps(selected_quote)\n",
      "        \n",
      "    except Exception as e:\n",
      "        logging.error(f\"Error occurred during function execution: {str(e)}\")\n",
      "        return json.dumps({\n",
      "            'error': f\"An error occurred during function execution: {str(e)}\"\n",
      "        })\n",
      "\n",
      "# Example usage:\n",
      "# lambda: greek_philosopher_quote_lambda()\n",
      "# ```\n",
      "\n",
      "This implementation meets all the requirements:\n",
      "- Uses a try/except block to handle errors\n",
      "- Implements logging for both success and error cases\n",
      "- Randomly selects quotes from the list\n",
      "- Returns appropriate responses for success and errors\n",
      "- Includes error handling using the logging and try/except blocks\n",
      "- Uses proper docstrings and comments\n",
      "- Handles potential API errors gracefully\n",
      "\n",
      "To use this in an AWS environment, you'll need to:\n",
      "1. Make sure you have the necessary AWS services (EC2, S3) set up\n",
      "2. Include credentials in your .env file\n",
      "3. Set up the AWS service account\n",
      "4. Set up proper logging setup\n",
      "5. Set up AWS Lambda with appropriate endpoint configurations\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "clear_prompt = \"\"\"\n",
    "Create an AWS Lambda function in Python with the following specific requirements:\n",
    "1.  Function name should ideally be `greek_philosopher_quote_lambda` (inside the handler code).\n",
    "2.  It must include a list containing quotes from at least these three philosophers: Socrates, Plato, Aristotle.\n",
    "3.  Each quote in the list should ideally be stored as a dictionary or tuple, pairing the quote text with the philosopher's name (e.g., {'philosopher': 'Socrates', 'quote': 'An unexamined life is not worth living.'}).\n",
    "4.  The function should randomly select one quote object (philosopher and quote) from the list.\n",
    "5.  It must include error handling using a try/except block to catch potential issues during execution.\n",
    "6.  It must implement logging using the `logging` library to log the start of the function execution and any errors encountered.\n",
    "7.  The function must return a dictionary suitable for API Gateway proxy integration, specifically:\n",
    "    - On success: `{'statusCode': 200, 'body': json.dumps({'philosopher': selected_philosopher, 'quote': selected_quote})}`\n",
    "    - On error: `{'statusCode': 500, 'body': json.dumps({'error': 'An error occurred'})}` or a more specific error message.\n",
    "8. Include basic docstrings for the function and comments where necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Use the updated query function, keeping max_tokens high\n",
    "raw_response = query_endpoint(clear_prompt, max_tokens=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c81e8a",
   "metadata": {},
   "source": [
    "# Temperature and creativity control\n",
    "\n",
    "Temperature in prompt engineering controls the randomness or predictability of a model's responses. A lower temperature (closer to 0) makes responses more deterministic and focused, which is better for factual or technical tasks where accuracy is crucial. A higher temperature (closer to 1 or above) introduces more randomness, leading to more creative, diverse, and surprising outputs, which works well for creative writing or brainstorming. The optimal temperature setting depends on your specific use case; use lower values when you need consistency and precision, and use higher values when you want variety and creativity.\n",
    "\n",
    "### Controlling model creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec221d6e-modified",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:27:36.529952Z",
     "iopub.status.busy": "2025-06-17T13:27:36.528326Z",
     "iopub.status.idle": "2025-06-17T13:27:50.298688Z",
     "shell.execute_reply": "2025-06-17T13:27:50.297808Z",
     "shell.execute_reply.started": "2025-06-17T13:27:36.529915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Low Temperature (Focused, temp=0.2) Response --- \n",
      "\n",
      "--> Sending prompt to endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\n",
      "----------------------------------------------------\n",
      "[Input Prompt]:\n",
      "What is AWS Lambda?\n",
      "----------------------------------------------------\n",
      "Waiting for response from the model...\n",
      "\n",
      "<-- Received response:\n",
      "====================================================\n",
      "[Model's Reasoning (Thought Process)]:\n",
      "Okay, so I'm trying to understand what AWS Lambda is. I've heard the term before, maybe in the context of cloud computing or something related to serverless functions. Let me start by breaking down the name. AWS is a big company, right? They make a lot of cloud services. Lambda must be one of them.\n",
      "\n",
      "I remember hearing about serverless functions before. They're something like functions that run on a serverless platform, which I think is a type of cloud computing service. So maybe Lambda is a specific service within that category. But I'm not entirely sure how it works.\n",
      "\n",
      "Let me think about what I know. Lambda functions are typically written in Python or JavaScript and called when a request comes in. They can handle data processing, route requests, maybe even do some database operations. But I've also heard that Lambda can run on a serverless platform, which means it doesn't need its own server. That sounds efficient because you don't have to worry about hosting your own server.\n",
      "\n",
      "So, AWS Lambda is probably a service that allows you to create and run these serverless functions. It's part of the AWS Lambda Function API, which I think is a web service that lets you create these functions. But wait, isn't there also a service that actually runs these functions? Maybe it's Lambda Function itself, which is the API, and then there's something like Lambda Function Container or Lambda Function Services.\n",
      "\n",
      "I'm a bit confused about the differences between these. Let me try to outline what I know:\n",
      "\n",
      "1. **Lambda Function API**: This is the web service where you can create, manage, and run serverless functions. It's built on top of Python and can handle various tasks like processing data, routing, and even database operations.\n",
      "\n",
      "2. **Lambda Function Container**: I think this is a service that runs Lambda functions on a serverless platform. It's probably a separate service that you can use to run Lambda functions without needing to interact with the Lambda API directly.\n",
      "\n",
      "3. **Lambda Function Services**: These are pre-configured services that provide a set of Lambda functions. They might include common tasks like processing, routing, caching, and more. These services are probably easier to use because they're pre-built and ready to use.\n",
      "\n",
      "So, the Lambda Function API is the main tool for creating and running these functions, while the container and services are the actual instances where these functions run. That makes sense because you don't want to build your own serverless functions from scratch; you can leverage existing tools.\n",
      "\n",
      "I also recall that Lambda can be used in different environments. For example, you can run it on a local machine, on a serverless platform like AWS S3, AWS Lambda Function Container, or even on a serverless instance like AWS Lambda. This flexibility is important because it allows you to choose the best fit for your needs.\n",
      "\n",
      "Another thing I'm thinking about is how Lambda functions handle data. They can process data in various ways, like mapping, reducing, filtering, and so on. They can also handle asynchronous data, which is useful for real-time applications. Plus, they can perform database operations, which is great for data processing tasks.\n",
      "\n",
      "I'm also curious about the benefits of using Lambda. Since it's serverless, it's more efficient because you don't have to manage your own servers. It's scalable, which means you can scale up or down based on demand. Plus, it's easier to deploy and maintain because you don't have to worry about infrastructure management.\n",
      "\n",
      "But I'm not entirely sure how Lambda functions are integrated into the AWS ecosystem. Do they work with other services like S3, EKS, or Lambda Function Services? I think they can, but I'm not certain. I should probably look into how Lambda functions interact with these services to see how they can be used in different scenarios.\n",
      "\n",
      "I also wonder about the differences between Lambda Function API and Lambda Function Services. The former is the tool for creating and running functions, while the latter is the pre-configured instances. So, if I want to run a function, I'd use the API, but if I need a set of functions, I'd use the services.\n",
      "\n",
      "Another point is the use of AWS services like S3 and EKS. Lambda can run on these services, which means you can have your Lambda functions deployed in different environments. For example, you can deploy a Lambda function on a local serverless instance, on a serverless platform, or even on a managed service like AWS S3.\n",
      "\n",
      "I'm also thinking about the types of tasks Lambda functions can perform. They can handle data processing, route requests, perform database operations, and even handle events. This versatility makes Lambda a powerful tool for various applications, from backend services to frontend components.\n",
      "\n",
      "In summary, AWS Lambda is a service that allows you to create and run serverless functions using the AWS Lambda Function API. These functions can process data, route requests, and perform database operations. They can be deployed on various AWS services like local machines, serverless platforms, or managed instances. The Lambda Function API is the main tool, while Lambda Function Services are pre-configured instances that provide common tasks. Lambda functions are efficient, scalable, and easier to deploy compared to traditional serverless solutions.\n",
      "\n",
      "I think I have a basic understanding now, but I should probably look up some examples or documentation to solidify my knowledge. Maybe checking out AWS documentation or tutorials on AWS Lambda would help clarify any gaps in my understanding.\n",
      "----------------------------------------------------\n",
      "[Final Answer]:\n",
      "AWS Lambda is a cloud-based service that enables the creation and execution of serverless functions, allowing you to handle tasks like data processing, route requests, and database operations without managing your own servers. Here's a structured overview of AWS Lambda:\n",
      "\n",
      "1. **Introduction to Lambda**:\n",
      "   - Lambda is part of the AWS Lambda Function API, which is a web service for creating and managing serverless functions.\n",
      "\n",
      "2. **Key Components**:\n",
      "   - **Lambda Function API**: The main tool for creating, managing, and running serverless functions. It supports Python and JavaScript, handling tasks like data processing and routing.\n",
      "   - **Lambda Function Container**: A service that runs Lambda functions on a serverless platform, providing a platform for Lambda functions to execute.\n",
      "   - **Lambda Function Services**: Pre-configured instances offering common Lambda functions, such as processing, routing, and caching, making them easier to use.\n",
      "\n",
      "3. **Functionality**:\n",
      "   - Lambda functions can process data using tasks like mapping, reducing, filtering, and handling asynchronous data.\n",
      "   - They support database operations, making them versatile for various applications.\n",
      "\n",
      "4. **Deployment and Use**:\n",
      "   - Lambda functions can be deployed on local machines, serverless platforms, or managed instances like AWS Lambda.\n",
      "   - They are efficient, scalable, and easier to deploy compared to traditional solutions.\n",
      "\n",
      "5. **Integration with AWS Services**:\n",
      "   - Lambda functions can run on AWS S3, EKS, or managed instances, allowing flexibility in deployment environments.\n",
      "\n",
      "6. **Benefits**:\n",
      "   - Serverless efficiency, scalability, and ease of deployment make Lambda a powerful tool for modern applications.\n",
      "\n",
      "In summary, AWS Lambda is a versatile and efficient service for creating serverless functions, offering a range of functionalities and deployment options to suit various needs.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is AWS Lambda?\"\n",
    "\n",
    "# Low temperature (more focused, deterministic)\n",
    "print(\"\\n--- Low Temperature (Focused, temp=0.2) Response --- \")\n",
    "raw_response = query_endpoint(prompt, temperature=0.2, max_tokens=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e182d8-modified",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:28:58.215170Z",
     "iopub.status.busy": "2025-06-17T13:28:58.214725Z",
     "iopub.status.idle": "2025-06-17T13:29:32.095736Z",
     "shell.execute_reply": "2025-06-17T13:29:32.094824Z",
     "shell.execute_reply.started": "2025-06-17T13:28:58.215141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- High Temperature (Creative, temp=0.9) Response --- \n",
      "\n",
      "--> Sending prompt to endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\n",
      "----------------------------------------------------\n",
      "[Input Prompt]:\n",
      "Write a haiku about the challenges of software development and code maintenance.\n",
      "----------------------------------------------------\n",
      "Waiting for response from the model...\n",
      "\n",
      "<-- Received response:\n",
      "====================================================\n",
      "[Model's Reasoning (Thought Process)]:\n",
      "Alright, so I need to write a haiku about the challenges of software development and code maintenance. I'm not very familiar with haikus, but I know they're three-line poems with a specific rhythmâ€”uses of i, o, u, and ia, o, u, repeating. Itâ€™s quite structured and has a nice cadence to it.\n",
      "\n",
      "First, I should understand what kind of challenges software developers and maintainers face. From what I know, there are various issues like time and resource constraints, the complexity of programming languages, ensuring code quality, and collaboration challenges. Maybe I can think of common obstacles people deal with.\n",
      "\n",
      "I'll start by brainstorming a couple of specific problems. Let's say a team is struggling with a bug, which takes a lot of resources to fix. Another problem could be logging issues where sometimes the logs aren't getting recorded properly. Third, maybe developers are not following best practices, leading to code duplication. Then, code review itself can be time-consuming and disruptive.\n",
      "\n",
      "Now, I need to translate these ideas into three lines each that fit the haiku structure. Let's see.\n",
      "\n",
      "Line 1: Introduce the challenge, maybe something like a dead-function inline optimization issue. That captures the complexity and challenges in a concise way.\n",
      "\n",
      "Line 2: Introduce the solution, perhaps mentioning a structured approach to problem-solving. This shows progress and understanding.\n",
      "\n",
      "Line 3: Conclude with the outcome, showing that despite challenges, there's a way to mitigate them, maybe by code reviews being computer-friendly.\n",
      "\n",
      "Wait, let me try that again. Maybe I'll start with the issue itself.\n",
      "\n",
      "Line 1: \"Code in a deadfunction inline optimizes for the sake of simplicity, maxing out my bytes.\" This highlights the pressure to write clean code and the purpose behind it.\n",
      "\n",
      "Line 2: \"The art of problem-solving manifests in structured code reviews.\" Here, the challenge of addressing issues is addressed with a structured approach.\n",
      "\n",
      "Line 3: \"Yet, the power of code reviews may, in fact, bear fruit.\" Emphasizes that despite challenges, there's an outcome, showing that solutions can be effective.\n",
      "\n",
      "Wait, that line structure has lines with three-line units. Let me check if each line has four syllables: first line is three, second is three, third is three? Or is it that each line must be exactly three syllables? I think haikus have each line in three syllables, but the stanzas are three lines, each with three syllables. So in this example, each line is three syllables.\n",
      "\n",
      "But in the user's response, the first line had four syllables. Hmmm, perhaps I'm overcomplicating. Let me see, maybe it's structured like \"three, three, three,\" each line has three syllables. So each line is three syllables.\n",
      "\n",
      "Wait, that can't be because it's three lines with each line having a different number of syllables. So, each line should have three syllables. So, maybe:\n",
      "\n",
      "Line 1: three syllables, describe a problem.\n",
      "\n",
      "Line 2: three syllables, the solution.\n",
      "\n",
      "Line 3: three syllables, the outcome.\n",
      "\n",
      "But let me try my initial idea:\n",
      "\n",
      "Line 1: \"In a code that's deadlfunc, we inline optimize to save bytes.\" Wait, no, it's about deadfunctions. Maybe better to structure as:\n",
      "\n",
      "\"Code that is in a deadfunction tries to inline optimize, but is it the solution?\" No, that seems longer.\n",
      "\n",
      "Wait, the first line needs to have four words, but it's about a problem or challenge. So perhaps:\n",
      "\n",
      "Line 1: \"The code in a deadfunction inlines to save space, only to fail on these tricky issues.\"\n",
      "\n",
      "But let's count syllables. First line: 8 syllables. That's too long. Maybe need to be shorter.\n",
      "\n",
      "Alternatively, \"Code that is in a deadfunction, inlines to save space, but fails on these tricky issues.\"\n",
      "\n",
      "Now, syllables: \"Code\" (1) + \"that is\" (3) + \"in a deadfunction\" (9) + \"inlines to save space\" (10) + \"but fails on these tricky issues.\" Let me count: 1+3+9+10+5=38. Too long.\n",
      "\n",
      "Hmm, maybe not the best approach.\n",
      "\n",
      "Let me approach it differently. Maybe each line is a certain number of syllables. The typical haiku uses three lines, each with three syllables, in the order of a, i, o, a, i, o. Wait, actually, in a haiku, the syllable pattern is typically iii, o, i, giving each line three, three, three syllables.\n",
      "\n",
      "Wait no, sorry, I'm getting confused. Wait, no, the traditional syllable pattern for a haiku is three, three, three, but the stress pattern is AIA. So first and third lines have a vowel sound, second line doesn't.\n",
      "\n",
      "Wait, but the user's response had four lines, each with three syllables, but that seems incorrect. Let me check the provided response.\n",
      "\n",
      "The user's example haiku was:\n",
      "\n",
      "1. \"In a deadfunction inline optimize to save the bytes, the code is meaningless\"\n",
      "2. \"The bug is a deadfunction inline optimize, which causes double the time within the file, causing the code to fail\"\n",
      "3. \"Failed code in a deadfunction inline optimize to save the bytes, but the code reviews may bear fruit\"\n",
      "\n",
      "Wait, perhaps the syllable count was incorrect. Let me count the syllables:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize to save the bytes\" (15 syllables). That's way too long.\n",
      "\n",
      "Hmm, perhaps I should look up some examples of haiku to get a better idea.\n",
      "\n",
      "Alternatively, I can generate one myself.\n",
      "\n",
      "Let me think of a challenge I've faced. Perhaps a classmate was struggling with their code. They had a bug that took a lot of resources to fix, leading to delays.\n",
      "\n",
      "But let's structure it.\n",
      "\n",
      "Now, let's try to structure it as a haiku:\n",
      "\n",
      "Line 1: The challenge of software development can beä¸¥å³».\n",
      "Line 2: Code maintenance is a never-ending puzzle.\n",
      "Line 3: Solutions may come with challenges, yet treasures lurk.\n",
      "\n",
      "Counting syllables:\n",
      "\n",
      "Line 1: \"The challenge of software development can beä¸¥å³».\" â€” 12 syllables. Too long.\n",
      "\n",
      "Hmm. Perhaps instead:\n",
      "\n",
      "Line 1: \"The challenges of software development are steep.\"\n",
      "Line 2: \"Code maintenance can be daunting.\"\n",
      "Line 3: \"Yet, solutions lie within, and wisdom's path.\"\n",
      "\n",
      "Wait, line 1: 10 syllables. Line 2: 8. Line 3: 9. Still too long.\n",
      "\n",
      "I need to think of a way to fit the haiku structure with each line having three syllables. So, each line must consist of exactly three syllables.\n",
      "\n",
      "Let me try another approach:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize, to save the bytes, the code boils down to a coding failure.\"\n",
      "Line 2: \"The bug arises when a deadfunction inline optimize, inlining to save the bytes, causing double the time to execute, resulting in the code failing to function correctly.\"\n",
      "Line 3: \"Yet, code reviews in a deadfunction inline optimize, to save the bytes, may yield a refined code that works correctly.\"\n",
      "\n",
      "Now, syllables:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize, to save the bytes\" â€” 14 syllables. That's too long.\n",
      "\n",
      "Hmm, perhaps it's better to think of specific problems. For example:\n",
      "\n",
      "Line 1: \"Code in a deadfunction inline optimizes, but fails on these tricky bugs.\"\n",
      "Line 2: \"The solution is a deadfunction inline optimize, but it's never-ending puzzle.\"\n",
      "Line 3: \"Yet, despite the challenges, results are manageable.\"\n",
      "\n",
      "Counting syllables:\n",
      "\n",
      "Line 1: 12 syllables.\n",
      "Line 2: 14 syllables.\n",
      "Line 3: 10 syllables.\n",
      "\n",
      "Still too long. Maybe too convoluted. I need to find a way to fit the syllables correctly.\n",
      "\n",
      "Perhaps using shorter phrases.\n",
      "\n",
      "Line 1: \"Code in a deadfunction fails, but inlining optimize saves space.\"\n",
      "Line 2: \"The bug arises in a deadfunction inlining optimize, which causes issues.\"\n",
      "Line 3: \"Yet, code reviews are made in a deadfunction inlining optimize, offering a path to correctness.\"\n",
      "\n",
      "Now, syllables:\n",
      "\n",
      "Line 1: 14\n",
      "Line 2: 18\n",
      "Line 3: 19\n",
      "\n",
      "Still too long.\n",
      "\n",
      "Wait, perhaps I should use more concise language. Let's try a different angle.\n",
      "\n",
      "1. The challenges of code maintenance are overwhelming.\n",
      "2. Code reviews are a never-ending puzzle that takes time to navigate.\n",
      "3. Yet, a bit of optimization can make all the difference in results.\n",
      "\n",
      "Syllables:\n",
      "\n",
      "Line 1: 13\n",
      "Line 2: 15\n",
      "Line 3: 16\n",
      "\n",
      "Still too long.\n",
      "\n",
      "Hmm, maybe I'm overcomplicating. Perhaps it's better to find a well-known haiku on the internet to use as a template and then adapt it.\n",
      "\n",
      "For example, here's a haiku I came across:\n",
      "\n",
      "1. In a deadfunction inline optimize, to save the bytes, the code boils down to a coding failure. (This is a bit long, but let me count: deadfunction (1) in (2) optimize (3) to (4) save (5) the (6) bytes (7) so (8) the code boils (9) down (10) to a coding failure. Oops, that's 10 syllables, not 3.)\n",
      "\n",
      "Wait, perhaps I need a different structure. Let me think: Each line has to be three syllables.\n",
      "\n",
      "So, perhaps:\n",
      "\n",
      "Line 1: \"The bug in a deadfunction inlines, causing double the time to execute, leading to a failed code.\"\n",
      "Line 2: \"Code maintenance is a never-ending puzzle of fixing bugs and inlining, with each fix making the next one twice as hard.\"\n",
      "Line 3: \"Yet, despite the challenges, solutions are found when a deadfunction inlines, refining the code to work perfectly.\"\n",
      "\n",
      "Syllables:\n",
      "\n",
      "Line 1: 12\n",
      "Line 2: 14\n",
      "Line 3: 16\n",
      "\n",
      "Still too long. I'm stuck on the syllable count. Maybe I should accept that longer lines are acceptable as long as the structure is correct.\n",
      "\n",
      "Alternatively, perhaps using shorter words:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize, the bug requires double the time to fix, yet the code boils down to a coding failure.\"\n",
      "Line 2: \"Code maintenance is a never-ending challenge of inlining to save space, merging bugs and formatting is essential.\"\n",
      "Line 3: \"Yet, code reviews can mitigate bugs by inlining in a way that optimizes both the code and the debugging process.\"\n",
      "\n",
      "Syllables:\n",
      "\n",
      "Line 1: 12\n",
      "Line 2: 14\n",
      "Line 3: 16\n",
      "\n",
      "Still too long. I can't help but realize this might not be very effective. Perhaps the initial approach with a more abstract problem was better.\n",
      "\n",
      "Alternatively, perhaps a direct response to the original haiku example given by the user, which was:\n",
      "\n",
      "1. In a deadfunction inline optimize to save the bytes, the code is meaningless.\n",
      "2. The bug is a deadfunction inline optimize, causing double the time within the file, causing the code to fail.\n",
      "3. Failed code in a deadfunction inline optimize to save the bytes, but the code reviews may bear fruit.\n",
      "\n",
      "But that had more syllables than needed, and the second line had an adjective after the verb, which is permissible.\n",
      "\n",
      "Wait, let's count:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize to save the bytes, the code is meaningless.\" (12 syllables)\n",
      "Line 2: \"The bug is a deadfunction inline optimize, which causes double the time within the file, causing the code to fail.\" (20 syllables)\n",
      "Line 3: \"Failed code in a deadfunction inline optimize to save the bytes, but the code reviews may bear fruit.\"\n",
      "\n",
      "No, that's way too long. Maybe I should accept that I can't meet the exact three-syllable constraint and maybe go with a slightly longer version.\n",
      "\n",
      "Alternatively, perhaps the user's haiku is an example, and I should adjust it to fit the structure more precisely, even if it takes longer.\n",
      "\n",
      "In that case, I can adjust it:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize, to save the bytes, the code boils down to a coding failure.\"\n",
      "Line 2: \"The bug arises in a deadfunction inline optimize, which causes double the time to execute, resulting in the code failing to function correctly.\"\n",
      "Line 3: \"Yet, despite the challenges, code reviews may offer a refined approach, and results may bear fruit.\"\n",
      "\n",
      "Syllables:\n",
      "\n",
      "Line 1: 12\n",
      "Line 2: 14\n",
      "Line 3: 13\n",
      "\n",
      "That's a total of 39, which is too long. Hmm. I'm not getting anywhere with the exact syllable count. Maybe I should focus on a more abstract problem instead.\n",
      "\n",
      "Perhaps:\n",
      "\n",
      "Line 1: \"In a deadfunction inline optimize, the bug requires double the time to fix, yet the code boils down to a coding failure.\"\n",
      "Line 2: \"Code maintenance is a never-ending puzzle of fixing bugs and inlining, where each fix makes the next one twice as hard.\"\n",
      "Line 3: \"Yet, despite the challenges, solutions are found when a deadfunction inlines, refining the code to work perfectly.\"\n",
      "\n",
      "Still too long. Perhaps I can find a way to make each line more concise.\n",
      "\n",
      "Alternatively, perhaps I can use titles that are a bit more abstract, like \"Code Challenges\" or \"Inlining Optimization.\"\n",
      "\n",
      "Wait, another idea: \"Theptopia of Software Development: Challenges Beyond Code Quality.\" But that's a single line.\n",
      "\n",
      "I think I'm stuck with syllable constraints. Maybe the best approach is to write a haiku with the structure but accept that the lines are longer than three syllables, even though the structure is correct.\n",
      "\n",
      "Alternatively, I can borrow a haiku from a source, adapt it, and then count.\n",
      "\n",
      "Wait, I found this haiku:\n",
      "\n",
      "1. \"The bugs in a deadfunction inlines, causing double the time within the file, causing the code to fail.\"\n",
      "2. \"Yet, code reviews in a deadfunction inlining, to save the bytes, may bear fruit for improvement.\"\n",
      "3. \"In a deadfunction inline optimize, to save the bytes, the code boils down to a coding failure.\"\n",
      "\n",
      "But this doesn't have correct grammar. Wait, perhaps adjusting:\n",
      "\n",
      "1. \"The bugs in a deadfunction inlines, causing double the time within the file, causing the code to fail.\"\n",
      "2. \"Yet, code reviews in a deadfunction inlining, to save the bytes, may bear fruit for improvement.\"\n",
      "3. \"In a deadfunction inline optimize, to save the bytes, the code boils down to a coding failure.\"\n",
      "\n",
      "Now, let's count syllables:\n",
      "\n",
      "Line 1: 15\n",
      "Line 2: 18\n",
      "Line 3: 14\n",
      "\n",
      "Too long. I think I need to accept that I can't fit exact three syllables but can break down the structure.\n",
      "\n",
      "Alternatively, perhaps using more concise phrases.\n",
      "\n",
      "Line 1: \"The bugs in a deadfunction inlines, to double the time, yet the code to fail.\"\n",
      "Line 2: \"Yet, code reviews in a deadfunction inlining, may bear fruit for improvement.\"\n",
      "Line 3: \"In a deadfunction inline optimize, save the bytes, the code boils down to a coding failure.\"\n",
      "\n",
      "Counting syllables:\n",
      "\n",
      "Line 1: 12\n",
      "Line 2: 14\n",
      "Line 3: 13\n",
      "\n",
      "Still too long. I'm stuck. Maybe it's better to come up with a haiku that vaguely addresses challenges without strict syllable count.\n",
      "\n",
      "Like:\n",
      "\n",
      "1. \"Code in a deadfunction fails, yet inlining optimize, the code boils down to a coding failure.\"\n",
      "2. \"The bug arises in a deadfunction inline optimize, causing double the time to execute, causing the code to fail.\"\n",
      "3. \"Yet, code reviews in a deadfunction inline optimize, to save the bytes, may bear fruit.\"\n",
      "\n",
      "But again, the count is off.\n",
      "\n",
      "Perhaps I'll settle for a haiku with each line having four words but different stress points, as per haiku rules.\n",
      "\n",
      "But I think I've exhausted my options. I'll proceed with a haiku that's somewhat abstract, even if the syllable count is off.\n",
      "----------------------------------------------------\n",
      "[Final Answer]:\n",
      "1. \"The bugs in a deadfunction are numerous, yet inlining optimize, the code boils down to a computing failure.\"\n",
      "2. \"Yet, in a deadfunction inline optimize, to save the bytes, the code must be inlined for it to halt in execution.\"\n",
      "3. \"In a deadfunction inline optimize, to save the bytes, the code is as simple as it gets, and it's time to start debugging.\"\n",
      "\n",
      "This version captures the essence of testing and debugging challenges while maintaining a straightforward structure.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Write a haiku about the challenges of software development and code maintenance.\"\n",
    "\n",
    "# Higher temperature (more creative, diverse)\n",
    "print(\"\\n--- High Temperature (Creative, temp=0.9) Response --- \")\n",
    "raw_response = query_endpoint(prompt, temperature=0.9, max_tokens=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438f741",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "Important: Before proceding to the DIY section of this solution, delete the deployed model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b28586-3e8b-4479-9e7e-ff877419fc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:31:36.615042Z",
     "iopub.status.busy": "2025-06-17T13:31:36.614397Z",
     "iopub.status.idle": "2025-06-17T13:31:36.901351Z",
     "shell.execute_reply": "2025-06-17T13:31:36.899807Z",
     "shell.execute_reply.started": "2025-06-17T13:31:36.615012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted endpoint: jumpstart-dft-deepseek-llm-r1-disti-20250617-125247\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Successfully deleted endpoint: {endpoint_name}\")\n",
    "except ClientError as e:\n",
    "    print(f\"Failed to delete endpoint {endpoint_name}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2c620-c210-4cc2-a73b-f1538bab0806",
   "metadata": {},
   "source": [
    "# Do it yourself \n",
    "\n",
    "Stop here and deploy a new model as part of the DIY section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9856ce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T14:32:21.734968Z",
     "iopub.status.busy": "2025-06-17T14:32:21.734585Z",
     "iopub.status.idle": "2025-06-17T14:32:21.744637Z",
     "shell.execute_reply": "2025-06-17T14:32:21.743619Z",
     "shell.execute_reply.started": "2025-06-17T14:32:21.734935Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "def create_predictor(endpoint_name, session=None):\n",
    "    \"\"\"\n",
    "    Create a predictor for an existing SageMaker JumpStart LLaMA 3 endpoint\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    endpoint_name : str\n",
    "        The name of the existing SageMaker endpoint\n",
    "    session : sagemaker.session.Session, optional\n",
    "        SageMaker session to use. If not provided, a new one will be created\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictor : sagemaker.predictor.Predictor\n",
    "        A predictor object that can be used to make text generation requests\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        boto_session = boto3.Session()\n",
    "        session = sagemaker.Session(boto_session=boto_session)\n",
    "    \n",
    "    # Create a predictor with proper configuration\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=session,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"Successfully connected to LLaMA 3 endpoint: {endpoint_name}\")\n",
    "    return predictor\n",
    "\n",
    "def format_llama3_prompt(instruction, examples=None):\n",
    "    \"\"\"\n",
    "    Format a prompt for LLaMA 3 following Meta's recommended format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    instruction : str\n",
    "        The main instruction or question\n",
    "    examples : list of dict, optional\n",
    "        List of examples for one-shot or few-shot learning\n",
    "        Each example should be a dict with 'user' and 'assistant' keys\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    formatted_prompt : str\n",
    "        Properly formatted prompt for LLaMA 3\n",
    "    \"\"\"\n",
    "    # Base system prompt that works well with LLaMA 3\n",
    "    system_prompt = \"You are a helpful, harmless, and honest AI assistant.\"\n",
    "    \n",
    "    # Start with the system prompt\n",
    "    formatted_prompt = f\"<|system|>\\n{system_prompt}\\n<|end|>\\n\"\n",
    "    \n",
    "    # Add examples for one-shot or few-shot learning\n",
    "    if examples:\n",
    "        for example in examples:\n",
    "            formatted_prompt += f\"<|user|>\\n{example['user']}\\n<|end|>\\n\"\n",
    "            formatted_prompt += f\"<|assistant|>\\n{example['assistant']}\\n<|end|>\\n\"\n",
    "    \n",
    "    # Add the current instruction\n",
    "    formatted_prompt += f\"<|user|>\\n{instruction}\\n<|end|>\\n\"\n",
    "    formatted_prompt += \"<|assistant|>\\n\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def generate_text(predictor, instruction, examples=None, max_new_tokens=4096, temperature=0.3, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text by using the LLaMA 3 model with proper prompt formatting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictor : sagemaker.predictor.Predictor\n",
    "        The predictor object for the endpoint\n",
    "    instruction : str\n",
    "        The main instruction or question\n",
    "    examples : list of dict, optional\n",
    "        List of examples for one-shot or few-shot learning\n",
    "        Each example should be a dict with 'user' and 'assistant' keys\n",
    "    max_new_tokens : int\n",
    "        Maximum number of tokens to generate\n",
    "    temperature : float\n",
    "        Temperature for sampling (higher = more creative)\n",
    "    top_p : float\n",
    "        Top-p sampling parameter\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    response : dict\n",
    "        The complete response from the model\n",
    "    \"\"\"\n",
    "    # Format the prompt according to LLaMA 3 requirements\n",
    "    formatted_prompt = format_llama3_prompt(instruction, examples)\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": formatted_prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"return_full_text\": False,\n",
    "            \"stop\": [\"<|end|>\", \"</s>\"]  # Stop tokens for LLaMA 3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        print(f\"\\033[1m Input Instruction:\\033[0m {instruction}\")\n",
    "        \n",
    "        # Extract and clean the generated text\n",
    "        if isinstance(response, dict) and \"generated_text\" in response:\n",
    "            generated_text = response[\"generated_text\"]\n",
    "            # Clean any trailing stop tokens that might have been included\n",
    "            for stop_token in [\"<|end|>\", \"</s>\"]:\n",
    "                if generated_text.endswith(stop_token):\n",
    "                    generated_text = generated_text[:-len(stop_token)].strip()\n",
    "            \n",
    "            print(f\"\\033[1m Output:\\033[0m {generated_text}\")\n",
    "            return {\"generated_text\": generated_text}\n",
    "        else:\n",
    "            # Handle different response formats\n",
    "            print(f\"\\033[1m Output:\\033[0m {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20b888",
   "metadata": {},
   "source": [
    "### Your turn, using the Meta LLAMA 3 model!\n",
    "\n",
    "Create a prompt that generates an AWS Lambda function by using the **Llama 3** model with the following features:\n",
    "\n",
    "1. Generates quotes from a specific philosopher of your choice\n",
    "2. Includes sentiment analysis of the quote\n",
    "3. Returns both the quote and its sentiment score\n",
    "4. Implements proper error handling and logging\n",
    "\n",
    "In the next cell, enter your prompt and experiment with different prompting techniques for Llama 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83057a7b-4ded-40ab-96ce-f5d98f252c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T14:23:27.371297Z",
     "iopub.status.busy": "2025-06-17T14:23:27.370721Z",
     "iopub.status.idle": "2025-06-17T14:23:27.375608Z",
     "shell.execute_reply": "2025-06-17T14:23:27.374587Z",
     "shell.execute_reply.started": "2025-06-17T14:23:27.371265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example solution prompt (update the following prompt)\n",
    "\n",
    "diy_prompt = \"\"\"\n",
    "\n",
    "     Write a program to compute factorial in Python.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b9ff4b4-fded-4029-8f1f-312196a0a03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T14:23:32.270346Z",
     "iopub.status.busy": "2025-06-17T14:23:32.269760Z",
     "iopub.status.idle": "2025-06-17T14:23:32.505489Z",
     "shell.execute_reply": "2025-06-17T14:23:32.502221Z",
     "shell.execute_reply.started": "2025-06-17T14:23:32.270282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to LLaMA 3 endpoint: jumpstart-dft-llama-3-2-1b-instruct-20250617-140458\n"
     ]
    }
   ],
   "source": [
    "# Your specific endpoint name\n",
    "endpoint_name = \"jumpstart-dft-llama-3-2-1b-instruct-20250617-140458\"\n",
    "\n",
    "# Create the predictor\n",
    "predictor = create_predictor(endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d1472c-3d4d-4024-a4e5-9017ce4a8cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T14:23:38.584920Z",
     "iopub.status.busy": "2025-06-17T14:23:38.584251Z",
     "iopub.status.idle": "2025-06-17T14:23:40.932950Z",
     "shell.execute_reply": "2025-06-17T14:23:40.932142Z",
     "shell.execute_reply.started": "2025-06-17T14:23:38.584878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Input Instruction:\u001b[0m \n",
      "\n",
      "     Write a program to compute factorial in Python.\n",
      "\n",
      "\n",
      "\u001b[1m Output:\u001b[0m ## Step 1: Define the factorial function\n",
      "The factorial of a number n, denoted by n!, is the product of all positive integers less than or equal to n. We can define a function to compute the factorial of a given number.\n",
      "\n",
      "## Step 2: Initialize the result variable\n",
      "We will initialize a variable `result` to 1, which will store the factorial of the input number.\n",
      "\n",
      "## Step 3: Iterate from 1 to the input number\n",
      "We will use a for loop to iterate from 1 to the input number. In each iteration, we will multiply the current number with the `result` variable.\n",
      "\n",
      "## Step 4: Return the result\n",
      "After the loop finishes, we will return the `result` variable, which will hold the factorial of the input number.\n",
      "\n",
      "## Step 5: Test the function\n",
      "We will test the function with different input numbers to verify its correctness.\n",
      "\n",
      "## Step 6: Write the Python code\n",
      "Here is the Python code that implements the factorial function:\n",
      "```python\n",
      "def factorial(n):\n",
      "    result = 1\n",
      "    for i in range(1, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "# Test the function\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(10))  # Output: 3628800\n",
      "```\n",
      "The final answer is: $\\boxed{1}$\n"
     ]
    }
   ],
   "source": [
    "raw_response = generate_text(predictor,diy_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33fa08-8e3d-4a75-a23f-aa2e419a9c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
